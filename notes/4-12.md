Theorem 7.7 If $\vec{\beta}$ is a computable bias sequence that converges to $\beta \in (0,1)$ and $R \in RAND_{\vec{\beta}}$, then $dim(R) = {\cal H}(\beta)$.

Proof. Assume the hypothesis. By Theorem 7.1 and Lemma 7.3, $dim(R) \le {\cal H}(\beta)$. To show that $dim(R) \ge {\cal H}(\beta)$, let $s \in [0, {\cal H}(\beta))$ be computable (or rational), and let $d$ be an algorithmic $s$-supergale. By Lemma 7.6, $S^\infty[d] $ has algorithmic $\vec\beta$-measure 0. Since $R \in RAND_{\vec{\beta}}$, this implies that $R \notin S^\infty[d]$. Since this holds for all computable $s \in [0, {\cal H}(\beta))$ and all algorithmic $s$-supergales $d$, it follows that $dim(R) \ge {\cal H}(\beta)$.

Cor 7.8 If $\vec{\beta}$ is a computable sequence of biases that converges to $\frac{1}{2}$ slowly enough that $\sum_{i=0}^{\infty} (\beta_i - \frac{1}{2})^2 
= \infty$, then

$$RAND_{\vec{\beta}} \subseteq DIM_1  \setminus RAND$$

Fact. Every sequence of dim 1 is close to a random

$$\limsup_{n \to \infty} \frac{\# bits of S[0\ldots n-1] and R[0\ldots n-1]}{n} = 0$$

Conj. In $\mathbb{R}^n$, every point of dim $n$ is close to a random point $x \in \mathbb{R}^n$.
